{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1\n",
    "\n",
    "#### Zipf's Law on the body text from the Enron emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the modules we need.\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import SoupStrainer\n",
    "import datetime\n",
    "from glob import glob\n",
    "import json\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import string\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Path to the raw data.\n",
    "raw_data_path = os.path.expanduser(\"~/midterm/data/enron/maildir\")\n",
    "print(raw_data_path)\n",
    "\n",
    "# Path to the preprocessed data file (may or may not exist).\n",
    "preprocessed_data_path = os.path.expanduser(\"~/midterm/data/enron/preprocessed_analysis1.json\")\n",
    "print(preprocessed_data_path)\n",
    "\n",
    "# Check to see if the file is already there.\n",
    "def have_preprocessed_data():\n",
    "    return os.path.isfile(preprocessed_data_path)\n",
    "\n",
    "print(\"Preprocessed data? \" + str(have_preprocessed_data()))\n",
    "    \n",
    "# Make sure the stopwords corpus is available.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get a sample of documents from the Enron mail dump.\n",
    "def get_enron_all_documents():\n",
    "    \n",
    "    # Save the working directory (for later restoration).\n",
    "    saved_path = os.getcwd()\n",
    "    print(saved_path)\n",
    "\n",
    "    # Get the paths to the data files.\n",
    "    # Use the \"all_documents\" directory as a sample of the entire corpus.\n",
    "    os.chdir(raw_data_path)\n",
    "    result = glob('**/all_documents/*')\n",
    "\n",
    "    # Restore the working directory.\n",
    "    os.chdir(saved_path)\n",
    "    print(os.getcwd())\n",
    "    \n",
    "    print(\"found \" + str(len(result)) + \" files\")\n",
    "    print(result[0:20])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "#wordlists = PlaintextCorpusReader(data_path, enron_files)\n",
    "#wordlists.fileids()\n",
    "\n",
    "#corpus = nltk.Text(wordlists.words())\n",
    "#print(type(corpus))\n",
    "\n",
    "# Only consider tokens that consist entirely of letters to be valid words.\n",
    "def is_valid_word(w):\n",
    "    if w in string.punctuation:\n",
    "        return False\n",
    "    \n",
    "    if not re.match(r'^[a-z]+$', w):\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "stopword_list = stopwords.words('english')\n",
    "boring_words = ['com', 'x', 'cc', 'bcc', 'www', 'mime', 'aol', 'ascii', 'http', 'p', 'charset', 'date', 'content', 'type']\n",
    "def is_interesting_word(w):\n",
    "    if w in stopword_list:\n",
    "        return False\n",
    "\n",
    "    if w in boring_words:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "#all_enron_words = wordlists.words()\n",
    "#enron_words = list([w.lower() for w in all_enron_words \n",
    "#                    if is_valid_word(w.lower()) and is_interesting_word(w.lower())])\n",
    "#print('First hundred interesting words in enron_words:')\n",
    "#print(enron_words[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the list of words from the raw data.\n",
    "def create_enron_word_list():\n",
    "    print(\"Reading words from the raw data\")\n",
    "    word_list = []\n",
    "    file_counter = 0\n",
    "    start = timer()\n",
    "    enron_files = get_enron_all_documents()\n",
    "    for fp in enron_files:\n",
    "        # Print some output every 1000 files so we can see visible progress.\n",
    "        file_counter += 1\n",
    "        if file_counter == 1000:\n",
    "            elapsed = timer()\n",
    "            print(str(elapsed - start) + \" - processing: \" + fp)\n",
    "            file_counter = 0\n",
    "    \n",
    "        lines = []\n",
    "        with open(os.path.join(raw_data_path, fp), mode=\"rt\", encoding=\"ISO-8859-1\") as f:\n",
    "            # There's a blank line between the headers and the body of the email.\n",
    "            # Start in the headers.\n",
    "            in_content = False\n",
    "            for line in f:\n",
    "                trimmed = line.strip()\n",
    "                if in_content and trimmed != '':\n",
    "                    lines.append(trimmed)\n",
    "\n",
    "                if trimmed == '':\n",
    "                    # Now we are in the body of the email.\n",
    "                    in_content = True\n",
    "        \n",
    "        # Join the lines up to form a single long line, then tokenize it.\n",
    "        raw = ' '.join(lines)\n",
    "        tokens = nltk.wordpunct_tokenize(raw)\n",
    "        text = nltk.Text(tokens)\n",
    "\n",
    "        # Extract the words which are NOT stopwords.\n",
    "        new_words = [w.lower() for w in text if is_valid_word(w.lower()) and (not w.lower() in stopword_list)]\n",
    "        word_list.extend(new_words)\n",
    "\n",
    "    end = timer()\n",
    "    print(\"Finished after \" + str(end - start) + \" seconds.\") \n",
    "    print(\"Read \" + str(len(word_list)) + \" files\")\n",
    "    return word_list\n",
    "\n",
    "# Restore from the preprocessed data file.\n",
    "def restore_enron_word_dict():\n",
    "    print(\"Restoring from: \" + preprocessed_data_path)\n",
    "    \n",
    "    result = {}\n",
    "    with open(preprocessed_data_path, 'rt') as f:\n",
    "        try:\n",
    "            result = json.load(f)\n",
    "        except ValueError:\n",
    "            result = {}\n",
    "\n",
    "    return result\n",
    "\n",
    "# Save to the preprocessed data file.\n",
    "def save_enron_word_dict(dict_to_save):\n",
    "    # Save it to a file so we don't have to process the full email text again.\n",
    "    print(\"Saving word counts dictionary to: \" + preprocessed_data_path)\n",
    "    with open(preprocessed_data_path, 'wt') as f:\n",
    "        json.dump(dict_to_save, f)\n",
    "        \n",
    "# Count the frequency of words.\n",
    "def count_words(word_list):  \n",
    "    result = {}\n",
    "    for w in word_list:\n",
    "        if w not in result:\n",
    "            result[w] = 1\n",
    "        else:\n",
    "            result[w] += 1\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enron_word_counts = {}\n",
    "if have_preprocessed_data():\n",
    "    # Read enron_words from the preprocessed file.\n",
    "    enron_word_counts = restore_enron_word_dict()\n",
    "\n",
    "else:\n",
    "    # Read the raw data.\n",
    "    enron_words = create_enron_word_list()\n",
    "    \n",
    "    # Count frequency of words.\n",
    "    enron_word_counts = count_words(enron_words)\n",
    "    print(len(enron_word_counts))\n",
    "\n",
    "    # Save to a preprocessed file for next time.\n",
    "    save_enron_word_dict(enron_word_counts)\n",
    "    \n",
    "# Now enron_word_counts should exist.\n",
    "print(len(enron_word_counts))\n",
    "print(enron_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create NLTK frequency distribution object.\n",
    "fd = FreqDist()\n",
    "\n",
    "# For each token in the relevant text, increment its counter.\n",
    "for key in enron_word_counts:\n",
    "    fd[key] = enron_word_counts[key]\n",
    "\n",
    "# Total number of samples.\n",
    "print(\"Number of words: \" + str(fd.N()))\n",
    "\n",
    "# Number of unique words.\n",
    "print(\"Unique words: \" + str(fd.B()))\n",
    "\n",
    "# Look at the top 20 words sorted by frequency.  Use a lambda function.\n",
    "print(\"Most common words:\")\n",
    "common_words = map(lambda p: str(p[0]) + ' ' + str(p[1]), fd.most_common(20))\n",
    "for cw in common_words:\n",
    "    print(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create rank list and frequency list (without numpy or pandas) for the Zipf's law plot.\n",
    "rank_list = []\n",
    "frequency_list = []\n",
    "rank = 0\n",
    "for x in fd.most_common():\n",
    "    word = x[0]\n",
    "    count = x[1]\n",
    "    rank += 1\n",
    "    print(\"Word='\" + word + \"', Rank=\" + str(rank) + \", Frequency=\" + str(count))\n",
    "    rank_list.append(rank)\n",
    "    frequency_list.append(count)\n",
    "\n",
    "print(rank_list[0:50])\n",
    "print(frequency_list[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot rank vs frequency on a log log plot and show the plot.\n",
    "plt.loglog(rank_list, frequency_list)\n",
    "plt.xlabel('frequency(f)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('rank(r)', fontsize=14, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
